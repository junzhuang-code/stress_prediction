{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS590 Data Science Sp21\n",
    "# In-class Competition: Stress Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Jun Zhuang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "### 0. About The Project\n",
    "In this competition, I employ several machine learning models to predict the stress level based on the survey data. The prediction is evaluated by RMSE (The smaller, the better).\n",
    "\n",
    "\n",
    "### 1. Dataset\n",
    "The dataset comes from Kaggle competition: <https://www.kaggle.com/c/stress-prediction/data>\n",
    "\n",
    "\n",
    "### 2. Preprocessing\n",
    "* **Features Selection**  \n",
    "At first, I employ the backward/forward selection with a given threshold (**ts**) that filters out the columns whose non-null value is lower than the given threshold. The result shows that using **ts=0.86** can maintain most features (filter one out) and achieves the best score in the leaderboard.  \n",
    "I also implement **Sparse Regression** to find out the top 10 important features by using glmnet. These features are *'hisp', 'hincome', 'fam_exp1_cv', 'fam_actions_cv_10', 'fam_discord_cv', 'child_social_media_time_cv', 'physical_activities_hr_cv', 'sitting_weekday_hour_cv', 'SEX_F', 'SEX_M'*. The result shows that the family discord caused by COVID-19, the time that child spends on social media per week, and female has a strong positive relationship with the stress, whereas the physical activities per week and house income are negative to the stress. I'm wondering whether or not female is easier to feel stressed when they realize that their children spend so much time on social media (father doesn't seem to care about this). On the other hand, physical activities help release stress. According to this information, I select these ten features to predict but unfortunately got a bad RMSE (**2.8197**) in local data. Thus, I decide not to submit this prediction.\n",
    "\n",
    "* **Fill the Null Values**  \n",
    "I fill the null values of selected columns with three methods: **mean, mode, and class-mean**. For the first two methods, I fill the null values by the mean/mode value of the current column. For the third method, I find out the mean value of each class and then fill the null values with their corresponding mean. The result shows that the third method suffers serious overfitting problems. The first method achieves the best performance and thus I use this method in the competition.\n",
    "\n",
    "* **Implement One-hot Embedding**  \n",
    "I implement one-hot embedding on *'SEX' and 'higheduc'* as these two features are categorical strings. \n",
    "\n",
    "* **Drop Unnecessary Columns**  \n",
    "After that, I drop some unnecessary columns such as *'ID', 'interview_date', 'pstr', 'train_id' or 'test_id'*.\n",
    "\n",
    "* **Normalization**  \n",
    "Lastly, I normalize all values within [0, 1]. I repeat the above-mentioned preprocessing procedure for both train data and test data.\n",
    "\n",
    "\n",
    "### 3. Methods\n",
    "In this competition, I attempt several base models as follows:  \n",
    "1. Generalized linear model (glm);\n",
    "2. Generalized boosted regression model (gbm);\n",
    "3. Random forest (rf);\n",
    "4. Support vector regression (svr);\n",
    "5. XGBoost (xgb);\n",
    "6. Ligthgbm (lgb);\n",
    "7. Neural Networks (cnn).\n",
    "\n",
    "\n",
    "* At first, I try glm, gbm, rf, and svr. For svr, I try the kernel with 'linear', 'poly', and 'rbf'. The '**poly**' kernel has better performance so that I use this kernel for svr in this competition. The result shows that the gradient boosting method (**gbm**) achieved better performance (lower RMSE). Thus, I decided to go in this direction and try xgb and lgb.  \n",
    "* For xgb and lgb, I apply **grid search** to find out the optimal parameters (presented in the script). The submission reveals that using lgb with **85 features** (fill nan by mean) achieves the best performance (**RMSE=2.74570**) so far.  \n",
    "* In the next step, I try two ensemble methods to boost the performance. At first, I use \"**stacking**\" method by stacking the top-3-performance model's predicted labels (lgb, svr, xgb) and then apply linear regression to generate the final prediction. This result (**RMSE=2.76409**) is worse than the previous one.  \n",
    "* Furthermore, I also propose a new ensemble method, **iterative averaging (IA)**, to achieve better performance in the public leaderboard. This method averages the top-n predicted labels at first and then iteratively replaces the worst one if the new generated label achieves better performance. The process repeats until the generated label converges. **The intuition is that the prediction from different base models may be close to the optimum. Iterative averaging of the top-n predictions helps approach the optimum better.** In this competition, I use the **top 3** predictions in this method and achieve the best ***RMSE=2.74244*** in the public leaderboard. The pseudocode is presended below:  \n",
    "```python\n",
    "def iterative_averaging(Top_n_pred, Y_gt):\n",
    "    \"\"\"\n",
    "    @topic: Ensemble method: iterative averaging (IA).\n",
    "    @input: \n",
    "        Top_n_pred (mxn): the list of predicted labels;\n",
    "        Y_gt (mx1): the ground truth label.\n",
    "    @return:\n",
    "        Y_pred_mean (mx1): the new predicted label.\n",
    "    \"\"\"\n",
    "    Y_pred_mean = mean(Top_n_pred) # Averages the top-n predicted labels\n",
    "    while gap > 1e-4:\n",
    "        # If the new generated label achieves better performance\n",
    "        if RMSE(Y_pred_mean) < RMSE(Top_n_pred).any(): \n",
    "            Top_n_pred.replace(Y_pred_mean) # Replaces the worst one\n",
    "            Y_pred_mean_new = mean(Top_n_pred) # New prediction\n",
    "            gap = RMSE(Y_pred_mean_new) - RMSE(Y_pred_mean)\n",
    "            Y_pred_mean = Y_pred_mean_new # Update the prediction\n",
    "     return Y_pred_mean\n",
    "```\n",
    "\n",
    "* Besides, I also employ cnn in this task. The RMSE without dropout is **12.4962** on the test set whereas using dropout=0.3 improves the RMSE to **2.9544**. Note that both results are good on the train/validation set but the RMSE gets worse on the test without dropout. This result indicates using dropout can largely mitigate the overfitting. However, RMSE=2.9544 is not good so that I decide not to consider CNN.\n",
    "\n",
    "\n",
    "### 4. Conclusion\n",
    "In this competition, I implement Sparse Regression to find out the important features related to the stress level and investigate several base machine learning models. Moreover, I propose a new ensemble method, **iterative averaging (IA)**, to achieve better performance in the public leaderboard.\n",
    "\n",
    "\n",
    "### Acknowledgments\n",
    " Thanks professor Mohler for his insightful suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "data_df1 = pd.read_csv('./data/train.csv')\n",
    "data_df2 = pd.read_csv('./data/test.csv')\n",
    "#print(\"train.csv: \\n\")\n",
    "#print(data_df1.info()) # 查看特征统计\n",
    "#print(data_df1.describe()) # 查看数据的描述性统计\n",
    "#data_df1.corr() # 展示变量之间的相关性\n",
    "#data_df1.head() # 查看数据前5行\n",
    "#print(\"test.csv: \\n\")\n",
    "#print(data_df2.info())\n",
    "#print(data_df2.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the label and id\n",
    "label = data_df1[\"pstr\"]\n",
    "#print(\"Classes distribution: \\n\", label.value_counts())\n",
    "#print(\"Exist null labels? Ans:\", label.isnull().any())\n",
    "#train_id = data_df1[\"train_id\"]\n",
    "test_id = data_df2[\"test_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions for preprocessing\n",
    "\n",
    "def select_cols_fillna(df, ts):\n",
    "    # Select the columns from dataframe (df) that the percentage of null values is lower than given threshold (ts), \n",
    "    # and fill the nan with the mode/mean of corresponding columns.\n",
    "    df = df.copy()\n",
    "    list_drop = []\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum()/len(df) < ts:\n",
    "            if df[column].dtypes == 'float64':\n",
    "                df[column].fillna(df[column].mean(), inplace=True) # fill the nan with column mean\n",
    "            else:\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True) # fill the nan with column mode\n",
    "        else:\n",
    "            list_drop.append(column)\n",
    "    return df, list_drop\n",
    "\n",
    "def select_cols_fillna_train(df, ts, label_col=\"pstr\", idx_col=\"train_id\"):\n",
    "    # Select the columns from dataframe (df) that the percentage of null values is lower than given threshold (ts), \n",
    "    # and fill the nan with the mean of corresponding class in this columns.\n",
    "    # ref: https://blog.csdn.net/u010383605/article/details/78879515\n",
    "    df = df.copy() \n",
    "    df.index = list(data_df1[idx_col]) # assign the index for dataframe\n",
    "    list_drop = []\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum()/len(df) < ts:\n",
    "            if df[column].dtypes == 'float64':\n",
    "                df_temp = df[[label_col, column]].groupby(label_col).mean() # 当前类别的均值表\n",
    "                na_ttable = df[column].isna() # true table of nan\n",
    "                na_label = list(df.loc[na_ttable, label_col]) # the label of nan\n",
    "                na_cm = df_temp.loc[na_label, column] # 依据上述标签，取均值表里对应的值(class mean)\n",
    "                na_cm.index = df.loc[na_ttable, column].index # change the index\n",
    "                df.loc[na_ttable, column] = na_cm\n",
    "                if df[column].isnull().sum() > 0: # check the nan value again\n",
    "                    df[column].fillna(df[column].mean(), inplace=True) # fill the nan with column mean\n",
    "            else:\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True) # fill the nan with column mode\n",
    "        else:\n",
    "            list_drop.append(column)\n",
    "    return df, list_drop\n",
    "\n",
    "def ohe_cols(df, list_ohe):\n",
    "    # Implement one-hot embedding on selected columns\n",
    "    df = df.copy()\n",
    "    for column in list_ohe:\n",
    "        tmp_col = pd.get_dummies(df[column], prefix=column) # generate one-hot embedding\n",
    "        df = pd.concat([df, tmp_col], axis=1) # concate the new ohe to original df\n",
    "        df.drop([column], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_cols(df, list_drop):\n",
    "    # Drop selected colums (list_drop) and return the new daraframe (df)\n",
    "    df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column in list_drop:\n",
    "            df.drop([column], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def normalize(df):\n",
    "    # Normalize the dataframe to [0,1].\n",
    "    for column in df.columns:\n",
    "        max_value = df[column].max()\n",
    "        min_value = df[column].min()\n",
    "        df[column] = (df[column] - min_value) / (max_value - min_value)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_age</th>\n",
       "      <th>ageyear</th>\n",
       "      <th>pamarriedliving</th>\n",
       "      <th>race4</th>\n",
       "      <th>race6</th>\n",
       "      <th>hisp</th>\n",
       "      <th>raceeth5</th>\n",
       "      <th>raceeth7</th>\n",
       "      <th>hincome</th>\n",
       "      <th>pamarital</th>\n",
       "      <th>...</th>\n",
       "      <th>walking_hour_per_day_2_cv</th>\n",
       "      <th>walking_hour_per_day_cv</th>\n",
       "      <th>walking_min_per_day_cv</th>\n",
       "      <th>SEX_F</th>\n",
       "      <th>SEX_M</th>\n",
       "      <th>higheduc_&lt; HS Diploma</th>\n",
       "      <th>higheduc_Bachelor</th>\n",
       "      <th>higheduc_HS Diploma/GED</th>\n",
       "      <th>higheduc_Post Graduate Degree</th>\n",
       "      <th>higheduc_Some College</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.636816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.636816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.636816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.804614</td>\n",
       "      <td>0.217457</td>\n",
       "      <td>0.188341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235401</td>\n",
       "      <td>0.208682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.759063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158926</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8522 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      interview_age  ageyear  pamarriedliving     race4     race6  hisp  \\\n",
       "0              0.76     0.75         0.000000  0.000000  0.000000   0.0   \n",
       "1              0.64     0.50         1.000000  0.333333  0.200000   0.0   \n",
       "2              0.66     0.50         1.000000  0.333333  0.200000   0.0   \n",
       "3              0.68     0.50         1.000000  0.333333  0.200000   0.0   \n",
       "4              0.08     0.00         0.804614  0.217457  0.188341   0.0   \n",
       "...             ...      ...              ...       ...       ...   ...   \n",
       "8517           0.58     0.50         1.000000  0.000000  0.000000   0.0   \n",
       "8518           0.84     0.75         1.000000  0.333333  0.200000   0.0   \n",
       "8519           0.44     0.25         1.000000  0.000000  0.000000   1.0   \n",
       "8520           0.46     0.50         1.000000  0.000000  0.000000   1.0   \n",
       "8521           0.50     0.50         1.000000  0.000000  0.000000   1.0   \n",
       "\n",
       "      raceeth5  raceeth7   hincome  pamarital  ...  walking_hour_per_day_2_cv  \\\n",
       "0     0.000000  0.000000  0.000000   0.000000  ...                   0.158926   \n",
       "1     0.250000  0.200000  0.636816   1.000000  ...                   0.158926   \n",
       "2     0.250000  0.200000  0.636816   1.000000  ...                   0.158926   \n",
       "3     0.250000  0.200000  0.636816   1.000000  ...                   0.158926   \n",
       "4     0.235401  0.208682  0.000000   0.759063  ...                   0.158926   \n",
       "...        ...       ...       ...        ...  ...                        ...   \n",
       "8517  0.000000  0.000000  1.000000   1.000000  ...                   0.312500   \n",
       "8518  0.250000  0.200000  1.000000   1.000000  ...                   0.158926   \n",
       "8519  0.500000  0.400000  0.000000   1.000000  ...                   0.158926   \n",
       "8520  0.500000  0.400000  0.000000   1.000000  ...                   0.158926   \n",
       "8521  0.500000  0.400000  0.000000   1.000000  ...                   0.158926   \n",
       "\n",
       "      walking_hour_per_day_cv  walking_min_per_day_cv  SEX_F  SEX_M  \\\n",
       "0                    0.065736                0.476275    0.0    1.0   \n",
       "1                    0.065736                0.476275    0.0    1.0   \n",
       "2                    0.065736                0.476275    0.0    1.0   \n",
       "3                    0.065736                0.476275    0.0    1.0   \n",
       "4                    0.065736                0.476275    0.0    1.0   \n",
       "...                       ...                     ...    ...    ...   \n",
       "8517                 0.065736                0.476275    1.0    0.0   \n",
       "8518                 0.065736                0.476275    0.0    1.0   \n",
       "8519                 0.130435                0.750000    1.0    0.0   \n",
       "8520                 0.043478                0.750000    1.0    0.0   \n",
       "8521                 0.065736                0.476275    1.0    0.0   \n",
       "\n",
       "      higheduc_< HS Diploma  higheduc_Bachelor  higheduc_HS Diploma/GED  \\\n",
       "0                       0.0                0.0                      0.0   \n",
       "1                       0.0                0.0                      1.0   \n",
       "2                       0.0                0.0                      1.0   \n",
       "3                       0.0                0.0                      1.0   \n",
       "4                       0.0                0.0                      0.0   \n",
       "...                     ...                ...                      ...   \n",
       "8517                    0.0                1.0                      0.0   \n",
       "8518                    0.0                0.0                      0.0   \n",
       "8519                    0.0                0.0                      0.0   \n",
       "8520                    0.0                0.0                      0.0   \n",
       "8521                    0.0                0.0                      0.0   \n",
       "\n",
       "      higheduc_Post Graduate Degree  higheduc_Some College  \n",
       "0                               0.0                    1.0  \n",
       "1                               0.0                    0.0  \n",
       "2                               0.0                    0.0  \n",
       "3                               0.0                    0.0  \n",
       "4                               1.0                    0.0  \n",
       "...                             ...                    ...  \n",
       "8517                            0.0                    0.0  \n",
       "8518                            1.0                    0.0  \n",
       "8519                            1.0                    0.0  \n",
       "8520                            1.0                    0.0  \n",
       "8521                            1.0                    0.0  \n",
       "\n",
       "[8522 rows x 85 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing for \"train.csv\"\n",
    "\n",
    "# 1. Fill nan value\n",
    "ts = 0.86 # 0.99/0.86/0.78/0.76/0.75\n",
    "df_fillna1, list_drop = select_cols_fillna(data_df1, ts)\n",
    "# The preproceesing below causes overfitting (abandon)\n",
    "#df_fillna1, list_drop = select_cols_fillna_train(data_df1, ts, label_col=\"pstr\", idx_col=\"train_id\")\n",
    "#print(\"Drop list: \\n\", list_drop)\n",
    "# 2. One-hot embedding\n",
    "list_ohe = [\"SEX\", \"higheduc\"]\n",
    "df_ohe1 = ohe_cols(df_fillna1, list_ohe)\n",
    "# 3. Drop selected columns\n",
    "list_drop2 = [\"ID\", \"interview_date\", \"pstr\", \"train_id\"]\n",
    "list_drop.extend(list_drop2)\n",
    "df_drop1 = drop_cols(df_ohe1, list_drop)\n",
    "# 4. Normalize the df to [0, 1]\n",
    "df_nm1 = normalize(df_drop1)\n",
    "#print(df_nm1.info())\n",
    "df_nm1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_age</th>\n",
       "      <th>ageyear</th>\n",
       "      <th>pamarriedliving</th>\n",
       "      <th>race4</th>\n",
       "      <th>race6</th>\n",
       "      <th>hisp</th>\n",
       "      <th>raceeth5</th>\n",
       "      <th>raceeth7</th>\n",
       "      <th>hincome</th>\n",
       "      <th>pamarital</th>\n",
       "      <th>...</th>\n",
       "      <th>walking_hour_per_day_2_cv</th>\n",
       "      <th>walking_hour_per_day_cv</th>\n",
       "      <th>walking_min_per_day_cv</th>\n",
       "      <th>SEX_F</th>\n",
       "      <th>SEX_M</th>\n",
       "      <th>higheduc_&lt; HS Diploma</th>\n",
       "      <th>higheduc_Bachelor</th>\n",
       "      <th>higheduc_HS Diploma/GED</th>\n",
       "      <th>higheduc_Post Graduate Degree</th>\n",
       "      <th>higheduc_Some College</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.473421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>0.473421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>0.473421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156959</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>0.473421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8352 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      interview_age  ageyear  pamarriedliving  race4  race6  hisp  raceeth5  \\\n",
       "0              0.46     0.50              1.0    0.0    0.0   0.0       0.0   \n",
       "1              0.48     0.50              1.0    0.0    0.0   0.0       0.0   \n",
       "2              0.50     0.50              1.0    0.0    0.0   0.0       0.0   \n",
       "3              0.16     0.00              1.0    0.0    0.0   0.0       0.0   \n",
       "4              0.18     0.00              1.0    0.0    0.0   0.0       0.0   \n",
       "...             ...      ...              ...    ...    ...   ...       ...   \n",
       "8347           0.84     0.75              1.0    0.0    0.0   0.0       0.0   \n",
       "8348           0.86     0.75              1.0    0.0    0.0   0.0       0.0   \n",
       "8349           0.88     0.75              1.0    0.0    0.0   0.0       0.0   \n",
       "8350           0.28     0.25              1.0    0.0    0.0   0.0       0.0   \n",
       "8351           0.32     0.25              1.0    0.0    0.0   0.0       0.0   \n",
       "\n",
       "      raceeth7  hincome  pamarital  ...  walking_hour_per_day_2_cv  \\\n",
       "0          0.0      0.5        1.0  ...                   0.156959   \n",
       "1          0.0      0.5        1.0  ...                   0.156959   \n",
       "2          0.0      0.5        1.0  ...                   0.375000   \n",
       "3          0.0      1.0        1.0  ...                   0.156959   \n",
       "4          0.0      1.0        1.0  ...                   0.156959   \n",
       "...        ...      ...        ...  ...                        ...   \n",
       "8347       0.0      1.0        1.0  ...                   0.156959   \n",
       "8348       0.0      1.0        1.0  ...                   0.156959   \n",
       "8349       0.0      1.0        1.0  ...                   0.125000   \n",
       "8350       0.0      1.0        1.0  ...                   0.156959   \n",
       "8351       0.0      1.0        1.0  ...                   0.062500   \n",
       "\n",
       "      walking_hour_per_day_cv  walking_min_per_day_cv  SEX_F  SEX_M  \\\n",
       "0                    0.086957                0.000000    1.0    0.0   \n",
       "1                    0.086957                0.473421    1.0    0.0   \n",
       "2                    0.064729                0.473421    1.0    0.0   \n",
       "3                    0.130435                0.000000    0.0    1.0   \n",
       "4                    0.043478                0.000000    0.0    1.0   \n",
       "...                       ...                     ...    ...    ...   \n",
       "8347                 0.043478                0.000000    0.0    1.0   \n",
       "8348                 0.043478                0.750000    0.0    1.0   \n",
       "8349                 0.064729                0.473421    0.0    1.0   \n",
       "8350                 0.217391                0.750000    1.0    0.0   \n",
       "8351                 0.064729                0.473421    1.0    0.0   \n",
       "\n",
       "      higheduc_< HS Diploma  higheduc_Bachelor  higheduc_HS Diploma/GED  \\\n",
       "0                       0.0                0.0                      1.0   \n",
       "1                       0.0                0.0                      1.0   \n",
       "2                       0.0                0.0                      1.0   \n",
       "3                       0.0                0.0                      0.0   \n",
       "4                       0.0                0.0                      0.0   \n",
       "...                     ...                ...                      ...   \n",
       "8347                    0.0                1.0                      0.0   \n",
       "8348                    0.0                1.0                      0.0   \n",
       "8349                    0.0                1.0                      0.0   \n",
       "8350                    0.0                0.0                      0.0   \n",
       "8351                    0.0                0.0                      0.0   \n",
       "\n",
       "      higheduc_Post Graduate Degree  higheduc_Some College  \n",
       "0                               0.0                    0.0  \n",
       "1                               0.0                    0.0  \n",
       "2                               0.0                    0.0  \n",
       "3                               1.0                    0.0  \n",
       "4                               1.0                    0.0  \n",
       "...                             ...                    ...  \n",
       "8347                            0.0                    0.0  \n",
       "8348                            0.0                    0.0  \n",
       "8349                            0.0                    0.0  \n",
       "8350                            1.0                    0.0  \n",
       "8351                            1.0                    0.0  \n",
       "\n",
       "[8352 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing for \"test.csv\"\n",
    "\n",
    "# 1. Fill nan value\n",
    "ts = 0.86 # 0.99/0.86/0.78/0.76/0.75\n",
    "df_fillna2, list_drop = select_cols_fillna(data_df2, ts)\n",
    "#print(\"Drop list: \\n\", list_drop)\n",
    "# 2. One-hot embedding\n",
    "list_ohe = [\"SEX\", \"higheduc\"]\n",
    "df_ohe2 = ohe_cols(df_fillna2, list_ohe)\n",
    "# 3. Drop selected columns\n",
    "list_drop2 = [\"ID\", \"interview_date\", \"test_id\"]\n",
    "list_drop.extend(list_drop2)\n",
    "df_drop2 = drop_cols(df_ohe2, list_drop)\n",
    "# 4. Normalize the df to [0, 1]\n",
    "df_nm2 = normalize(df_drop2)\n",
    "df_nm2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine classic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Generalized linear model (glm)\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "def GLM(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the generalized linear model\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained gl model\n",
    "    \"\"\"\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# Generalized boosted regression model (gbrm)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "def GBRM(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the generalized boosted regression model\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained gbr model\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(random_state=0)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# Random forest regressor (rfr)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "def RFR(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the random forest regressor\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained rfr\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(max_depth=2, random_state=0) # max_depth: the maximum depth of the tree\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# SVM regression model (svrm)\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html\n",
    "def SVRM(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the SVM regression model\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained svr model\n",
    "    \"\"\"\n",
    "    #model = SVR(kernel='rbf', C=1.0, gamma=0.1, epsilon=.1)\n",
    "    #model = SVR(kernel='linear', C=1.0, gamma='auto')\n",
    "    model = SVR(kernel='poly', C=1.0, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement 2-Folds on train.csv for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train & X_test are (4261, 85), (4261, 85).\n"
     ]
    }
   ],
   "source": [
    "# Implement 2-Folds on train.csv for cross-validation\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "def KFolds(data, label, K=2):\n",
    "    # Split dataset (np.array) and label (np.array) into K folds (int)\n",
    "    assert len(data) == len(label)\n",
    "    kf = KFold(n_splits=K, random_state=0, shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(data):\n",
    "        #print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "        X_train, X_test = data[train_idx], data[test_idx]\n",
    "        Y_train, Y_test = label[train_idx], label[test_idx]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "K = 2 # K-Folds\n",
    "data_ar = df_nm1.to_numpy()\n",
    "label_ar = label.to_numpy()\n",
    "X_train, Y_train, X_test, Y_test = KFolds(data_ar, label_ar, K=2)\n",
    "print(\"The shape of X_train & X_test are {0}, {1}.\".format(X_train.shape, X_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GLM: \n",
      "RMSE: 2.8156.\n",
      "2-Folds CV Score (RMSE): [5.38419504e+04 1.69090423e+00].\n",
      "--------------------\n",
      "Model GBRM: \n",
      "RMSE: 2.7996.\n",
      "2-Folds CV Score (RMSE): [1.66702436 1.68178799].\n",
      "--------------------\n",
      "Model RFR: \n",
      "RMSE: 2.8180.\n",
      "2-Folds CV Score (RMSE): [1.66182899 1.68828573].\n",
      "--------------------\n",
      "Model SVRM: \n",
      "RMSE: 2.8019.\n",
      "2-Folds CV Score (RMSE): [1.66358412 1.69157387].\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Fitting given models and evaluation\n",
    "\n",
    "def fit_evaluate(Model, X_train, Y_train, X_test, Y_test, K=2):\n",
    "    # Fitting the given model and evaluate with RMSE\n",
    "    # Fit the model\n",
    "    model = Model(X_train, Y_train)\n",
    "    # Generate the predicted label on X_test\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Evaluation by root mean squared error (rmse)\n",
    "    print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred))))\n",
    "    # Present the K-Folds cross validation scores (rmse)\n",
    "    # sorted(sklearn.metrics.SCORERS.keys()) # check the scoring function\n",
    "    print(\"{0}-Folds CV Score (RMSE): {1}.\".\\\n",
    "          format(K, np.sqrt(list(-cross_val_score(model, X_train, Y_train, cv=K, \n",
    "                                                  scoring='neg_root_mean_squared_error')))))\n",
    "\n",
    "model_dict = {\"GLM\":GLM, \"GBRM\":GBRM, \"RFR\":RFR, \"SVRM\":SVRM}\n",
    "for model_name, model in model_dict.items():\n",
    "    print(\"Model {0}: \".format(model_name))\n",
    "    fit_evaluate(model, X_train, Y_train, X_test, Y_test, K)\n",
    "    print(\"-\"*20)\n",
    "# GBRM[(ts)RMSE]: (0.61)2.8027, (0.71)2.7992, (0.72/0.73)2.7953, (0.74)2.7975, (0.76)2.7946, (0.78)2.7924, (0.86)2.7911\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Y_pred based on test.csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train.csv & test.csv are (8522, 85), (8352, 85).\n",
      "Output the submission done!\n"
     ]
    }
   ],
   "source": [
    "# Generate Y_pred based on test.csv for submission\n",
    "\n",
    "def fit_predict(Model, X_train, Y_train, X_test):\n",
    "    # Fitting the given model and generate the predicted labels\n",
    "    model = Model(X_train, Y_train) # Fit the model\n",
    "    Y_pred = model.predict(X_test) # Generate the predicted labels\n",
    "    return Y_pred\n",
    "\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "X_te = df_nm2.to_numpy() # test.csv\n",
    "print(\"The shape of train.csv & test.csv are {0}, {1}.\".format(X_tr.shape, X_te.shape))\n",
    "\n",
    "# Use the model with the best performance to generate the prediction\n",
    "# model_dict = {\"GLM\":GLM, \"GBRM\":GBRM, \"RFR\":RFR}\n",
    "Y_pred = fit_predict(GBRM, X_tr, Y_tr, X_te)\n",
    "#Y_pred = fit_predict(RFR, X_tr, Y_tr, X_te)\n",
    "\n",
    "# Output the submission\n",
    "submission_df = pd.DataFrame({'test_id': test_id, 'pstr': Y_pred})\n",
    "submission_df.to_csv('./junzhuang_submission.csv', index=False)\n",
    "print(\"Output the submission done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration in the gradient boosting direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Grid Search to tune the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In previous experiment, I found that the gradient boosting method achieved better result.\n",
    "# In the upcoming experiment, I decide go along this direction and try xgb and lgb.\n",
    "\n",
    "# Implement grid search to search the best parameters\n",
    "def GridSearch(Model, cv_params, other_params, K, X_train, Y_train):\n",
    "    model = Model(**other_params)\n",
    "    optimized_model = GridSearchCV(estimator=model,\n",
    "                                   param_grid=cv_params, # Parameter for search\n",
    "                                   cv=K, # K-Folds\n",
    "                                   n_jobs=2) # Num_threads\n",
    "    optimized_model.fit(X_train, Y_train)\n",
    "    #print('The result in each iteration: {0}.'.format(optimized_model.grid_scores_))\n",
    "    print('Best parameters: {0}.'.format(optimized_model.best_params_))\n",
    "    print('Best score: {0}.'.format(optimized_model.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best parameters: {'eta': 0.01}.\n",
      "Best score: 0.04765794042494553.\n"
     ]
    }
   ],
   "source": [
    "# Implement grid search on xgb.XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "#cv_params = {'n_estimators': np.linspace(50, 100, 25, dtype=int)} # 81\n",
    "#cv_params = {'max_depth': np.linspace(1, 10, 10, dtype=int)} # 2\n",
    "#cv_params = {'min_child_weight': np.linspace(1, 10, 10, dtype=int)} # 1\n",
    "#cv_params = {'gamma': np.linspace(0, 1, 11)} # 0\n",
    "#cv_params = {'subsample': np.linspace(0, 1, 11)} # 1.0\n",
    "#cv_params = {'colsample_bytree': np.linspace(0, 1, 11)[1:]} # 1\n",
    "#cv_params = {'colsample_bylevel': np.linspace(0, 1, 11)[1:]} # 1\n",
    "#cv_params = {'reg_lambda': np.linspace(0, 1, 11)} # 0.3\n",
    "#cv_params = {'reg_alpha': np.linspace(0, 1, 11)} # 0.0\n",
    "cv_params = {'eta': np.logspace(-3, -2, 10)} # 0.01\n",
    "other_params = {'eta': 0.01, 'n_estimators': 81, 'max_depth': 2, 'min_child_weight': 1, 'gamma': 0, 'subsample': 1,\n",
    "                 'colsample_bytree': 1, 'colsample_bylevel': 1, 'reg_lambda': 0.3, 'reg_alpha': 0, 'seed': 0}         \n",
    "K = 5\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "GridSearch(xgb.XGBRegressor, cv_params, other_params, K, X_tr, Y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 183}.\n",
      "Best score: 0.05322937368214222.\n"
     ]
    }
   ],
   "source": [
    "# Implement grid search on lgb.LGBMRegressor (preprocessing: mode/mean)\n",
    "import lightgbm as lgb\n",
    "\n",
    "cv_params = {'n_estimators':  np.linspace(180, 220, 40, dtype=int)} # 225/225/741/216/198\n",
    "#cv_params = {'learning_rate': np.logspace(-2, -1, 100)} # 0.06428073117284322/0.0774263682681127/0.07924828983539177/0.05857020818056667/0.08697490026177834\n",
    "#cv_params = {'num_leaves': np.linspace(1, 10, 10, dtype=int)} # 3\n",
    "#cv_params = {'max_depth': np.linspace(1, 10, 10, dtype=int)} # 2\n",
    "#cv_params = {'min_data_in_leaf': np.linspace(1, 20, 20, dtype=int)} # 11/11/11/7/8\n",
    "#cv_params = {'min_sum_hessian_in_leaf': np.logspace(-4, -3, 10)} # 0.0001\n",
    "#cv_params = {'min_gain_to_split': np.linspace(0, 1, 11)[1:]} # 1.0/1.0/0.3/0.1/0.3\n",
    "#cv_params = {'lambda_l2': np.linspace(0, 2, 21)} # 1.6/1.5/1.3/0.6/1.9\n",
    "#cv_params = {'bagging_fraction': np.linspace(0, 1, 11)[1:]} # 0.5\n",
    "#cv_params = {'bagging_freq': np.linspace(1, 10, 10, dtype=int)} # 3\n",
    "#cv_params = {'feature_fraction': np.linspace(0, 1, 11)[1:]} # 0.6/0.6/0.6/0.9/0.9\n",
    "#cv_params = {'feature_fraction_bynode': np.linspace(0, 1, 11)[1:]} # 0.5\n",
    "\n",
    "other_params = {'objective': 'regression',\n",
    "               'n_estimators': 225,\n",
    "               'learning_rate': 0.0774263682681127,\n",
    "               'num_leaves': 3,\n",
    "               'max_depth': 2,\n",
    "               'min_data_in_leaf': 11,\n",
    "               'min_sum_hessian_in_leaf': 0.0001,\n",
    "               'min_gain_to_split': 1.0,\n",
    "               'lambda_l2': 1.5,\n",
    "               'bagging_fraction': 0.5,\n",
    "               'bagging_freq': 3,\n",
    "               'feature_fraction': 0.6,\n",
    "               'feature_fraction_bynode': 0.5\n",
    "                }\n",
    "K=5\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "GridSearch(lgb.LGBMRegressor, cv_params, other_params, K, X_tr, Y_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting, prediction, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.7904.\n",
      "5-Folds CV Score (RMSE): [1.67404797 1.66314256 1.63270445 1.68016116 1.6816074 ].\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# https://blog.csdn.net/jh1137921986/article/details/84754868\n",
    "# https://blog.csdn.net/weixin_41789280/article/details/104463363\n",
    "# https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_intro.html#install-xgboost\n",
    "def XGB(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the xgboost regressor\n",
    "    @input: X_train/Y_train: train data/label (array); best_params: dict of the best parameters;\n",
    "    @return: model: trained xgbr\n",
    "    \"\"\"\n",
    "    model = xgb.XGBRegressor(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "best_params = {'objective': 'reg:squarederror', 'eta': 0.01, 'n_estimators': 81, \n",
    "               'max_depth': 2, 'min_child_weight': 1, 'gamma': 0, 'subsample': 1,\n",
    "               'colsample_bytree': 1, 'colsample_bylevel': 1, 'reg_lambda': 0.3, 'reg_alpha': 0, 'seed': 0}\n",
    "xgbr = XGB(X_train, Y_train, best_params)\n",
    "# Generate the predicted label on X_test\n",
    "Y_pred = xgbr.predict(X_test)\n",
    "# Evaluation by root mean squared error (rmse)\n",
    "print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred))))\n",
    "# Present the K-Folds cross validation scores (rmse)\n",
    "K=5\n",
    "print(\"{0}-Folds CV Score (RMSE): {1}.\".\\\n",
    "          format(K, np.sqrt(list(-cross_val_score(xgb.XGBRegressor(**best_params), data_ar, label_ar, cv=K, \n",
    "                                                  scoring='neg_root_mean_squared_error')))))\n",
    "# 2.7958/2.7912\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.7879.\n",
      "5-Folds CV Score (RMSE): [1.6733841  1.66639579 1.62756563 1.68060491 1.67609128].\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "def LGBMR(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the lightgbm regressor\n",
    "    @input: X_train/Y_train: train data/label (array); best_params: dict of the best parameters;\n",
    "    @return: model: trained lgbmr\n",
    "    \"\"\"\n",
    "    model = lgb.LGBMRegressor(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "best_params = {'objective': 'regression',\n",
    "           'n_estimators': 225,\n",
    "           'learning_rate': 0.0774263682681127,\n",
    "           'num_leaves': 3,\n",
    "           'max_depth': 2,\n",
    "           'min_data_in_leaf': 11,\n",
    "           'min_sum_hessian_in_leaf': 0.0001,\n",
    "           'min_gain_to_split': 1.0,\n",
    "           'lambda_l2': 1.5,\n",
    "           'bagging_fraction': 0.5,\n",
    "           'bagging_freq': 3,\n",
    "           'feature_fraction': 0.6,\n",
    "           'feature_fraction_bynode': 0.5\n",
    "            }\n",
    "\n",
    "lgbmr = LGBMR(X_train, Y_train, best_params)\n",
    "# Generate the predicted label on X_test\n",
    "Y_pred = lgbmr.predict(X_test)\n",
    "# Evaluation by root mean squared error (rmse)\n",
    "print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred))))\n",
    "# Present the K-Folds cross validation scores (rmse)\n",
    "K=5\n",
    "print(\"{0}-Folds CV Score (RMSE): {1}.\".\\\n",
    "          format(K, np.sqrt(list(-cross_val_score(lgb.LGBMRegressor(**best_params), data_ar, label_ar, cv=K, \n",
    "                                                  scoring='neg_root_mean_squared_error')))))\n",
    "# 2.7959/2.7903/2.7828/2.7879(#f=85)/0.4027(overfitting)/2.7867(#f=86)/2.7891(#f=82)\n",
    "# Conclusion: using \"mean\" to fill the nan achieves better performance.\n",
    "# 小结：预处理时，用mean替代mode来填充空值，测试集表现更佳；用各类别均值分别填充时会导致过拟合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train.csv & test.csv are (8522, 85), (8352, 85).\n",
      "Output the submission done!\n"
     ]
    }
   ],
   "source": [
    "# Generate Y_pred based on test.csv for submission\n",
    "\n",
    "def fit_predict(Model, best_params, X_train, Y_train, X_test):\n",
    "    # Fitting the given model and generate the predicted labels\n",
    "    model = Model(X_train, Y_train, best_params) # Fit the model\n",
    "    Y_pred = model.predict(X_test) # Generate the predicted labels\n",
    "    return Y_pred\n",
    "\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "X_te = df_nm2.to_numpy() # test.csv\n",
    "print(\"The shape of train.csv & test.csv are {0}, {1}.\".format(X_tr.shape, X_te.shape))\n",
    "\n",
    "# Generate the prediction from XGB\n",
    "model_name = \"LGBMR\"\n",
    "if model_name == \"XGB\":\n",
    "    best_params = {'objective': 'reg:squarederror', 'eta': 0.01, 'n_estimators': 81, \n",
    "                   'max_depth': 2, 'min_child_weight': 1, 'gamma': 0, 'subsample': 1,\n",
    "                   'colsample_bytree': 1, 'colsample_bylevel': 1, 'reg_lambda': 0.3, 'reg_alpha': 0, 'seed': 0}\n",
    "    Y_pred = fit_predict(XGB, best_params, X_tr, Y_tr, X_te)\n",
    "if model_name == \"LGBMR\":\n",
    "    best_params = {'objective': 'regression',\n",
    "                   'n_estimators': 225,\n",
    "                   'learning_rate': 0.0774263682681127,\n",
    "                   'num_leaves': 3,\n",
    "                   'max_depth': 2,\n",
    "                   'min_data_in_leaf': 11,\n",
    "                   'min_sum_hessian_in_leaf': 0.0001,\n",
    "                   'min_gain_to_split': 1.0,\n",
    "                   'lambda_l2': 1.5,\n",
    "                   'bagging_fraction': 0.5,\n",
    "                   'bagging_freq': 3,\n",
    "                   'feature_fraction': 0.6,\n",
    "                   'feature_fraction_bynode': 0.5\n",
    "                    }\n",
    "    Y_pred = fit_predict(LGBMR, best_params, X_tr, Y_tr, X_te)\n",
    "\n",
    "# Output the submission\n",
    "assert len(Y_pred) != 0\n",
    "submission_df = pd.DataFrame({'test_id': test_id, 'pstr': Y_pred})\n",
    "submission_df.to_csv('./junzhuang_submission.csv', index=False)\n",
    "print(\"Output the submission done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1.0, 'coef0': 1.1, 'degree': 3, 'epsilon': 0.212, 'gamma': 'auto', 'kernel': 'poly', 'tol': 0.001}.\n",
      "Best score: -2.793249110811356.\n"
     ]
    }
   ],
   "source": [
    "# Implement grid search on SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "tuned_para = [{'kernel': ['poly'], # ['linear', 'poly', 'rbf'],\n",
    "               #'degree': [2, 3, 4],\n",
    "               #'coef0': [1.0, 1.1, 1.2],\n",
    "               #'tol': np.logspace(-3, -2, 5), # 0.001\n",
    "               #'C':  np.linspace(0.8, 1.2, 5, dtype=float), # 0.21\n",
    "               #'epsilon': np.linspace(0.20, 0.23, 6, dtype=float), # 2.0\n",
    "               'degree': [3],\n",
    "               'gamma': ['auto'],\n",
    "               'coef0': [1.1],\n",
    "               'tol': [0.001],\n",
    "               'C': [1.0],\n",
    "               'epsilon': [0.212]\n",
    "              }]\n",
    "\n",
    "K = 3\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "clf = GridSearchCV(estimator=SVR(),\n",
    "                   param_grid=tuned_para,\n",
    "                   scoring='neg_root_mean_squared_error',\n",
    "                   cv=K)\n",
    "clf.fit(X_tr, Y_tr)\n",
    "print('Best parameters: {0}.'.format(clf.best_params_))\n",
    "print('Best score: {0}.'.format(clf.best_score_))\n",
    "# linear:2.7862493529445698; poly:2.793249110811356.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.8022.\n",
      "3-Folds CV Score (RMSE): [1.6835077  1.63974325 1.6902045 ].\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def SVRM(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the SVM regression model\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained svr model\n",
    "    \"\"\"\n",
    "    model = SVR(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "\"\"\"\n",
    "optimal_para = {'kernel': 'linear',\n",
    "                 'gamma': 'auto',\n",
    "                 'tol': 0.0001,\n",
    "                 'C': 0.21, \n",
    "                 'epsilon': 2.0\n",
    "               }\n",
    "\"\"\"\n",
    "optimal_para = {'kernel': 'poly',\n",
    "                'degree': 3,\n",
    "                'gamma': 'auto',\n",
    "                'coef0': 1.1,\n",
    "                'tol': 0.001,\n",
    "                'C': 1.0, \n",
    "                'epsilon': 0.212\n",
    "               }\n",
    "svrm = SVRM(X_train, Y_train, optimal_para)\n",
    "# Generate the predicted label on X_test\n",
    "Y_pred = svrm.predict(X_test)\n",
    "# Evaluation by root mean squared error (rmse)\n",
    "print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred))))\n",
    "# Present the K-Folds cross validation scores (rmse)\n",
    "K=3\n",
    "print(\"{0}-Folds CV Score (RMSE): {1}.\".\\\n",
    "          format(K, np.sqrt(list(-cross_val_score(SVR(**optimal_para), data_ar, label_ar, cv=K, \n",
    "                                                  scoring='neg_root_mean_squared_error')))))\n",
    "# RMSE: 2.8094/2.8022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train.csv & test.csv are (8522, 85), (8352, 85).\n",
      "Output the submission done!\n"
     ]
    }
   ],
   "source": [
    "# Generate Y_pred based on test.csv for submission\n",
    "def fit_predict(Model, best_params, X_train, Y_train, X_test):\n",
    "    # Fitting the given model and generate the predicted labels\n",
    "    model = Model(X_train, Y_train, best_params) # Fit the model\n",
    "    Y_pred = model.predict(X_test) # Generate the predicted labels\n",
    "    return Y_pred\n",
    "\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "X_te = df_nm2.to_numpy() # test.csv\n",
    "print(\"The shape of train.csv & test.csv are {0}, {1}.\".format(X_tr.shape, X_te.shape))\n",
    "\n",
    "# Generate the prediction\n",
    "optimal_para = {'kernel': 'poly',\n",
    "                'degree': 3,\n",
    "                'gamma': 'auto',\n",
    "                'coef0': 1.1,\n",
    "                'tol': 0.001,\n",
    "                'C': 1.0, \n",
    "                'epsilon': 0.212\n",
    "               }\n",
    "Y_pred = fit_predict(SVRM, optimal_para, X_tr, Y_tr, X_te)\n",
    "\n",
    "# Output the submission\n",
    "assert len(Y_pred) != 0\n",
    "submission_df = pd.DataFrame({'test_id': test_id, 'pstr': Y_pred})\n",
    "submission_df.to_csv('./junzhuang_submission.csv', index=False)\n",
    "print(\"Output the submission done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods -- Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train.csv & test.csv are (8522, 85), (8352, 85).\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on both train.csv and test.csv by given models\n",
    "# ref: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def LGBMR(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the lightgbm regressor\n",
    "    @input: X_train/Y_train: train data/label (array); best_params: dict of the best parameters;\n",
    "    @return: model: trained lgbmr\n",
    "    \"\"\"\n",
    "    model = lgb.LGBMRegressor(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def XGB(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the xgboost regressor\n",
    "    @input: X_train/Y_train: train data/label (array); best_params: dict of the best parameters;\n",
    "    @return: model: trained xgbr\n",
    "    \"\"\"\n",
    "    model = xgb.XGBRegressor(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def SVRM(X_train, Y_train, best_params):\n",
    "    \"\"\"\n",
    "    @topic: Fitting the SVM regression model\n",
    "    @input: X_train/Y_train: train data/label (array)\n",
    "    @return: model: trained svr model\n",
    "    \"\"\"\n",
    "    model = SVR(**best_params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def fit_predict(Model, best_params, X_train, Y_train, X_test):\n",
    "    # Fitting the given model and generate the predicted labels\n",
    "    model = Model(X_train, Y_train, best_params) # Fit the model\n",
    "    Y_pred = model.predict(X_test) # Generate the predicted labels\n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "# Define the dataset\n",
    "X_tr = df_nm1.to_numpy() # train.csv\n",
    "Y_tr = label.to_numpy()\n",
    "X_te = df_nm2.to_numpy() # test.csv\n",
    "print(\"The shape of train.csv & test.csv are {0}, {1}.\".format(X_tr.shape, X_te.shape))\n",
    "\n",
    "# Generate the prediction\n",
    "# XGB\n",
    "xgb_params = {'objective': 'reg:squarederror', 'eta': 0.01, 'n_estimators': 81, \n",
    "              'max_depth': 2, 'min_child_weight': 1, 'gamma': 0, 'subsample': 1,\n",
    "              'colsample_bytree': 1, 'colsample_bylevel': 1, 'reg_lambda': 0.3, 'reg_alpha': 0, 'seed': 0}\n",
    "Y_pred_xgb_tr = fit_predict(XGB, xgb_params, X_tr, Y_tr, X_tr) # Y_pred for train.csv\n",
    "Y_pred_xgb_te = fit_predict(XGB, xgb_params, X_tr, Y_tr, X_te) # Y_pred for test.csv\n",
    "# LGB\n",
    "lgb_params = {'objective': 'regression',\n",
    "              'n_estimators': 225,\n",
    "              'learning_rate': 0.0774263682681127,\n",
    "              'num_leaves': 3,\n",
    "              'max_depth': 2,\n",
    "              'min_data_in_leaf': 11,\n",
    "              'min_sum_hessian_in_leaf': 0.0001,\n",
    "              'min_gain_to_split': 1.0,\n",
    "              'lambda_l2': 1.5,\n",
    "              'bagging_fraction': 0.5,\n",
    "              'bagging_freq': 3,\n",
    "              'feature_fraction': 0.6,\n",
    "              'feature_fraction_bynode': 0.5\n",
    "              }\n",
    "Y_pred_lgb_tr = fit_predict(LGBMR, lgb_params, X_tr, Y_tr, X_tr) # Y_pred for train.csv\n",
    "Y_pred_lgb_te = fit_predict(LGBMR, lgb_params, X_tr, Y_tr, X_te) # Y_pred for test.csv\n",
    "# SVR\n",
    "svr_para = {'kernel': 'poly',\n",
    "            'degree': 3,\n",
    "            'gamma': 'auto',\n",
    "            'coef0': 1.1,\n",
    "            'tol': 0.001,\n",
    "            'C': 1.0, \n",
    "            'epsilon': 0.212\n",
    "            }\n",
    "Y_pred_svr_tr = fit_predict(SVRM, svr_para, X_tr, Y_tr, X_tr) # Y_pred for train.csv\n",
    "Y_pred_svr_te = fit_predict(SVRM, svr_para, X_tr, Y_tr, X_te) # Y_pred for test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction based on the ensemble stacking method\n",
    "from sklearn import linear_model\n",
    "\n",
    "def reshape(A):\n",
    "    # reshape numpy array from (len(A),) to (len(A),1)\n",
    "    return A.reshape((len(A),-1))\n",
    "\n",
    "# Concate the predicted labels\n",
    "Y_en_tr = np.concatenate((reshape(Y_pred_xgb_tr), reshape(Y_pred_lgb_tr), reshape(Y_pred_svr_tr)), axis=1)\n",
    "Y_en_te = np.concatenate((reshape(Y_pred_xgb_te), reshape(Y_pred_lgb_te), reshape(Y_pred_svr_te)), axis=1)\n",
    "\n",
    "# Train a regression model\n",
    "glm_en = linear_model.LinearRegression()\n",
    "glm_en.fit(Y_en_tr, Y_tr)\n",
    "Y_pred = glm_en.predict(Y_en_te)\n",
    "# Conlucion: the score is not good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output the submission done!\n"
     ]
    }
   ],
   "source": [
    "# Output the submission\n",
    "assert len(Y_pred) != 0\n",
    "submission_df = pd.DataFrame({'test_id': test_id, 'pstr': Y_pred})\n",
    "submission_df.to_csv('./junzhuang_submission.csv', index=False)\n",
    "print(\"Output the submission done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse regression (Lasso regression) using glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t df \t %dev \t lambdau\n",
      "\n",
      "0 \t 0.000000 \t 0.000000 \t 0.300000\n",
      "1 \t 0.000000 \t 0.000000 \t 0.290000\n",
      "2 \t 1.000000 \t 0.000384 \t 0.280000\n",
      "3 \t 4.000000 \t 0.001545 \t 0.270000\n",
      "4 \t 4.000000 \t 0.003455 \t 0.260000\n",
      "5 \t 4.000000 \t 0.005292 \t 0.250000\n",
      "6 \t 4.000000 \t 0.007058 \t 0.240000\n",
      "7 \t 4.000000 \t 0.008751 \t 0.230000\n",
      "8 \t 4.000000 \t 0.010372 \t 0.220000\n",
      "9 \t 5.000000 \t 0.012291 \t 0.210000\n",
      "10 \t 6.000000 \t 0.014311 \t 0.200000\n",
      "11 \t 7.000000 \t 0.016472 \t 0.190000\n",
      "12 \t 8.000000 \t 0.018647 \t 0.180000\n",
      "13 \t 8.000000 \t 0.020960 \t 0.170000\n",
      "14 \t 9.000000 \t 0.023204 \t 0.160000\n",
      "15 \t 10.000000 \t 0.025492 \t 0.150000\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junzhuang/anaconda3/lib/python3.7/site-packages/glmnet_python/glmnetPlot.py:211: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later\n",
      "  ax2.set(XLim=[min(index), max(index)], XTicks = atdf, XTickLabels = prettydf)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'fig': <Figure size 432x288 with 2 Axes>,\n",
       " 'ax1': <AxesSubplot:xlabel='Log Lambda', ylabel='Coefficients'>,\n",
       " 'ax2': <AxesSubplot:label='6e886ff5-c8e5-453c-bfb6-1121e1a32d11', xlabel='Degrees of Freedom'>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a sparse regression to the competition data using glmnet with alpha=1\n",
    "# ref: https://glmnet-python.readthedocs.io/en/latest/glmnet_vignette.html#Linear-Regression\n",
    "import glmnet_python\n",
    "from glmnet import glmnet\n",
    "from glmnetPrint import glmnetPrint\n",
    "from glmnetPlot import glmnetPlot\n",
    "\n",
    "# Define the dataset\n",
    "data_ar = df_nm1.to_numpy()\n",
    "label_ar64 = label.to_numpy().astype(np.float64)\n",
    "# Tune lambda so that you end up with only 10 coefficients that are non-zero.\n",
    "lambda_list = np.array([0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30])\n",
    "#lambda_list = np.linspace(0.15,0.30,16)\n",
    "fit = glmnet(x = data_ar, y = label_ar64, alpha = 1.0, lambdau=lambda_list, nlambda = 20)\n",
    "# Print out the 10 non-zero coef (df)\n",
    "print(glmnetPrint(fit))\n",
    "# Plotting with labels\n",
    "glmnetPlot(fit, xvar = 'lambda', label = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 8, 13, 30, 36, 62, 64, 72, 79, 80]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['hisp', 'hincome', 'fam_exp1_cv', 'fam_actions_cv___10',\n",
       "       'fam_discord_cv', 'child_social_media_time_cv',\n",
       "       'child_texting_time_2_cv', 'physical_activities_hr_cv', 'SEX_F',\n",
       "       'SEX_M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the non-zero coefficients\n",
    "from glmnetCoef import glmnetCoef\n",
    "lambda_10 = 0.15 # Select the lambda value that meets 10 non-zero coefficients\n",
    "coef_list = glmnetCoef(fit, s = np.float64([lambda_10]), exact = False)\n",
    "coef_list10 = [i-1 for i in range(len(coef_list)) if coef_list[i] != 0][1:]\n",
    "print(coef_list10)\n",
    "# [5, 8, 13, 30, 36, 62, 64, 72, 79, 80]\n",
    "# 10 variables that those correspond to non-zero coefficients\n",
    "df_nm1.columns[coef_list10]\n",
    "# According to the plotting, 'hincome', 'physical_activities_hr_cv' and 'SEX_M' are negatively correlated with stress \n",
    "# and rest variables have positively correlation.\n",
    "# Note that SEX_M and SEX_F are two features after 'SEX' one-hot embedding.\n",
    "# Conclusion: Female is easier to feel stressed when they realize that their children spend so much time on social media (father doesn't seem to care about this). On the other hand, physical activities help release stress. \n",
    "# 小结：（由疫情导致的）家庭不和谐，孩子娱乐时间，女性这三变量与压力成正比（女性压力明显比男性要大）；运动量和收入与压力成反比（明显地，运动越多越能缓解压力）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.8197.\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "from glmnetPredict import glmnetPredict\n",
    "Y_pred = glmnetPredict(fit, newx = X_test, s=np.float64([lambda_10]))\n",
    "print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred))))\n",
    "# The result is pretty bad. Not to submit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the neural network for regression \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# ref: https://machinelearningmastery.com/prediction-intervals-for-deep-learning-neural-networks/\n",
    "\n",
    "# Define neural network model\n",
    "def CNN(n_feat, lr):\n",
    "    # Implement CNN for regression\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, kernel_initializer='he_normal', activation='relu', input_dim=n_feat))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    # Compile the model and specify loss and optimizer\n",
    "    opt = SGD(learning_rate=lr, momentum=0.0, nesterov=False)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6817 samples, validate on 1705 samples\n",
      "Epoch 1/50\n",
      "6817/6817 - 2s - loss: 10.7955 - val_loss: 8.4517\n",
      "Epoch 2/50\n",
      "6817/6817 - 1s - loss: 8.3682 - val_loss: 8.4537\n",
      "Epoch 3/50\n",
      "6817/6817 - 1s - loss: 8.1171 - val_loss: 8.4421\n",
      "Epoch 4/50\n",
      "6817/6817 - 1s - loss: 8.0530 - val_loss: 8.4459\n",
      "Epoch 5/50\n",
      "6817/6817 - 1s - loss: 8.0444 - val_loss: 8.4416\n",
      "Epoch 6/50\n",
      "6817/6817 - 1s - loss: 8.0257 - val_loss: 8.5080\n",
      "Epoch 7/50\n",
      "6817/6817 - 1s - loss: 8.0241 - val_loss: 8.4627\n",
      "Epoch 8/50\n",
      "6817/6817 - 1s - loss: 8.0249 - val_loss: 8.4434\n",
      "Epoch 9/50\n",
      "6817/6817 - 1s - loss: 8.0178 - val_loss: 8.4912\n",
      "Epoch 10/50\n",
      "6817/6817 - 1s - loss: 8.0175 - val_loss: 8.4626\n",
      "Epoch 11/50\n",
      "6817/6817 - 1s - loss: 8.0188 - val_loss: 8.4500\n",
      "Epoch 12/50\n",
      "6817/6817 - 1s - loss: 8.0142 - val_loss: 8.4649\n",
      "Epoch 13/50\n",
      "6817/6817 - 1s - loss: 8.0225 - val_loss: 8.4947\n",
      "Epoch 14/50\n",
      "6817/6817 - 1s - loss: 8.0143 - val_loss: 8.5213\n",
      "Epoch 15/50\n",
      "6817/6817 - 1s - loss: 8.0147 - val_loss: 8.4703\n",
      "Epoch 16/50\n",
      "6817/6817 - 1s - loss: 8.0089 - val_loss: 8.5254\n",
      "Epoch 17/50\n",
      "6817/6817 - 1s - loss: 8.0145 - val_loss: 8.4402\n",
      "Epoch 18/50\n",
      "6817/6817 - 1s - loss: 7.9990 - val_loss: 8.4356\n",
      "Epoch 19/50\n",
      "6817/6817 - 1s - loss: 8.0058 - val_loss: 8.4291\n",
      "Epoch 20/50\n",
      "6817/6817 - 1s - loss: 8.0068 - val_loss: 9.1102\n",
      "Epoch 21/50\n",
      "6817/6817 - 1s - loss: 8.0326 - val_loss: 8.4485\n",
      "Epoch 22/50\n",
      "6817/6817 - 1s - loss: 8.0113 - val_loss: 8.4402\n",
      "Epoch 23/50\n",
      "6817/6817 - 1s - loss: 8.0124 - val_loss: 8.4410\n",
      "Epoch 24/50\n",
      "6817/6817 - 1s - loss: 8.0159 - val_loss: 8.4543\n",
      "Epoch 25/50\n",
      "6817/6817 - 1s - loss: 8.0086 - val_loss: 8.4519\n",
      "Epoch 26/50\n",
      "6817/6817 - 1s - loss: 8.0164 - val_loss: 8.4636\n",
      "Epoch 27/50\n",
      "6817/6817 - 1s - loss: 8.0048 - val_loss: 8.5113\n",
      "Epoch 28/50\n",
      "6817/6817 - 1s - loss: 8.0083 - val_loss: 8.4391\n",
      "Epoch 29/50\n",
      "6817/6817 - 1s - loss: 8.0065 - val_loss: 8.4901\n",
      "Epoch 30/50\n",
      "6817/6817 - 1s - loss: 8.0016 - val_loss: 8.5273\n",
      "Epoch 31/50\n",
      "6817/6817 - 1s - loss: 7.9994 - val_loss: 8.5035\n",
      "Epoch 32/50\n",
      "6817/6817 - 1s - loss: 7.9884 - val_loss: 8.4233\n",
      "Epoch 33/50\n",
      "6817/6817 - 1s - loss: 8.0026 - val_loss: 8.4564\n",
      "Epoch 34/50\n",
      "6817/6817 - 1s - loss: 8.0031 - val_loss: 8.4357\n",
      "Epoch 35/50\n",
      "6817/6817 - 1s - loss: 8.0029 - val_loss: 8.5907\n",
      "Epoch 36/50\n",
      "6817/6817 - 1s - loss: 7.9890 - val_loss: 8.4211\n",
      "Epoch 37/50\n",
      "6817/6817 - 1s - loss: 7.9867 - val_loss: 8.9845\n",
      "Epoch 38/50\n",
      "6817/6817 - 1s - loss: 8.0120 - val_loss: 8.4041\n",
      "Epoch 39/50\n",
      "6817/6817 - 1s - loss: 7.9817 - val_loss: 8.4402\n",
      "Epoch 40/50\n",
      "6817/6817 - 1s - loss: 7.9615 - val_loss: 8.4009\n",
      "Epoch 41/50\n",
      "6817/6817 - 1s - loss: 7.9750 - val_loss: 8.6057\n",
      "Epoch 42/50\n",
      "6817/6817 - 1s - loss: 7.9756 - val_loss: 8.6311\n",
      "Epoch 43/50\n",
      "6817/6817 - 1s - loss: 7.9538 - val_loss: 8.4126\n",
      "Epoch 44/50\n",
      "6817/6817 - 1s - loss: 7.9553 - val_loss: 8.6056\n",
      "Epoch 45/50\n",
      "6817/6817 - 1s - loss: 7.9385 - val_loss: 8.4102\n",
      "Epoch 46/50\n",
      "6817/6817 - 1s - loss: 7.9701 - val_loss: 8.4090\n",
      "Epoch 47/50\n",
      "6817/6817 - 1s - loss: 7.9756 - val_loss: 8.3682\n",
      "Epoch 48/50\n",
      "6817/6817 - 1s - loss: 7.9560 - val_loss: 8.3882\n",
      "Epoch 49/50\n",
      "6817/6817 - 1s - loss: 7.9763 - val_loss: 8.6173\n",
      "Epoch 50/50\n",
      "6817/6817 - 1s - loss: 7.9598 - val_loss: 8.3628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4983f2e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "X = df_nm1.to_numpy() # train.csv\n",
    "Y = label.to_numpy()\n",
    "n_feat = X.shape[1]\n",
    "lr = 0.01\n",
    "# Initialize the model\n",
    "cnn = CNN(n_feat, lr)\n",
    "# Fit the model on the train data\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "cnn.fit(X, Y, epochs=N_EPOCHS, batch_size=BATCH_SIZE, \n",
    "        validation_split=0.2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.8576.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "Y_pred_ = cnn.predict(X_test, verbose=0) # X_te or X_test\n",
    "# Calculate the average error in the predictions\n",
    "print(\"RMSE: {:.4f}.\".format(np.sqrt(mean_squared_error(Y_test, Y_pred_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output the submission done!\n"
     ]
    }
   ],
   "source": [
    "# Output the submission\n",
    "X_te = df_nm2.to_numpy() # test.csv\n",
    "Y_pred = cnn.predict(X_te, verbose=0)\n",
    "assert len(Y_pred) != 0\n",
    "submission_df = pd.DataFrame({'test_id': test_id, 'pstr': Y_pred.flatten()})\n",
    "submission_df.to_csv('./junzhuang_submission.csv', index=False)\n",
    "print(\"Output the submission done!\")\n",
    "# RMSE(test.csv) with dropout=0.3: 2.95444\n",
    "# RMSE(test.csv) without dropout: 12.49624\n",
    "# Conlusion: using dropout can largely mitigate the overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
